{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing a roll call pdf\n",
    "> Extracting voting information from roll call vote PDF files (those with RCV in their file name), which were downloaded via `eu_parliament.download`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some useful references:**\n",
    "\n",
    "- PyPDF2 docs: https://pythonhosted.org/PyPDF2/PdfFileReader.html\n",
    "    - `PageObject`: https://pythonhosted.org/PyPDF2/PageObject.html#PyPDF2.pdf.PageObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content of RCV pdfs:**\n",
    "- intro\n",
    "- table of contents\n",
    "- votes by issue (split by outcome +/-/0, i.e. yes/no/abstain and party):\n",
    "    - every MEP is represented by the surname\n",
    "    - each issue may also have vote corrections (only MEP and no party info and for some reasonthe MEPs's names here include their forename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** \n",
    "- Some MEP names seem to disappear when reading the PDF, like Kolakušić, and some are incorrectly read like Ždanoka $\\rightarrow$ ”danoka $\\rightarrow$ fix in `SummaryParser` / `VotesParser` or PyPDF2\n",
    "    - consequence: the total vote count and the counted names of the actually parsed MEPs disagrees\n",
    "    - tried `pdfminer`, which initially looked promising (it's not dropping words as far as I can see) but actually copies and pasts bits of texts all over the place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import PyPDF2 as pdf\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import re\n",
    "import pandas as pd\n",
    "import collections\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from eu_parliament import download\n",
    "from pdfminer import high_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing available PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_date_from_filename(file): return pd.to_datetime(re.search(r'(\\d{4}-\\d{2}-\\d{2})', file).group())\n",
    "\n",
    "def list_available_pdfs(file_dir:typing.Union[str, Path]=download.PDF_PATH, \n",
    "                        filter_fun:typing.Callable=lambda x: 'RCV' in x):\n",
    "    \n",
    "    pdf_files = [Path(file_dir)/file for file in os.listdir(file_dir) if filter_fun(file)]\n",
    "    file_timestamps = [extract_date_from_filename(file.name) for file in pdf_files]\n",
    "    \n",
    "    return pd.DataFrame({'file': pdf_files, 'timestamp': file_timestamps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv_pdfs = list_available_pdfs(); rcv_pdfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = rcv_pdfs.iloc[0,0]; path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Pdf2Text:\n",
    "    'Parsing a PDF to a list of strings'\n",
    "    \n",
    "    def __call__(self, path:typing.Union[Path,str], \n",
    "                 method:str='pdfminer', **kwargs):\n",
    "        \n",
    "        fun = getattr(self, f'parse_with_{method}')\n",
    "        return fun(path, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_with_pypdf2(path:typing.Union[Path,str]):\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            reader = pdf.PdfFileReader(f)\n",
    "\n",
    "            all_texts = [reader.getPage(i).extractText()  \n",
    "                         for i in range(reader.getNumPages())]\n",
    "        return all_texts\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_with_pdfminer(path:typing.Union[Path,str],\n",
    "                            codec:str='UTF-8'):\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            string = high_level.extract_text(f, codec=codec)\n",
    "            \n",
    "        all_texts = re.split(r'(PE\\s\\d{3}\\.\\d{3})', string)\n",
    "        all_texts = [s0+s1 for s0,s1 in zip(all_texts[::2], all_texts[1::2])]\n",
    "        return all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pdfminer` to parse the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_texts = Pdf2Text()(path, method='pdfminer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert isinstance(all_texts, list)\n",
    "assert all([isinstance(text, str) for text in all_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `PyPDF2` to parse the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_texts2 = Pdf2Text()(path, method='pypdf2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert isinstance(all_texts2, list)\n",
    "assert all([isinstance(text, str) for text in all_texts2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(all_texts) == len(all_texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_texts2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the extracted texts we see that PyPDF2 seems to drop some words whereas pdfminer does not seem to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining two utility functions to make our lifes easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ix(strings:typing.List[str], pattern_fun:typing.Callable): # , must_match:bool=True\n",
    "    return [i for i, s in enumerate(strings) if pattern_fun(s)] # (must_match and s==pattern) or (not must_match and pattern in s)\n",
    "\n",
    "def useful_string(s):  return ('docx' not in s) and (len(s)>0) \\\n",
    "            and (s != ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SummaryParser:\n",
    "    'Parsing the summary of the issues voted on and documented in an RCV file'\n",
    "    \n",
    "    #pattern = re.compile('(\\.\\.+)')\n",
    "    is_toc = lambda self, text: 'SOMMAIRE' in text or 'CONTENTS' in text\n",
    "    \n",
    "    def __init__(self, all_texts:typing.List[str],\n",
    "                 method:str):\n",
    "        assert method in ['pdfminer', 'pypdf2']\n",
    "        \n",
    "        self.method = method\n",
    "        self.num_pages = len(all_texts)\n",
    "        \n",
    "        self.ix_start, summary_text = self.get_start(all_texts)\n",
    "\n",
    "        self.vote_names, self.vote_page_numbers = self.parse_names_and_page_numbers(summary_text)\n",
    "        \n",
    "        self.ix_end, summary_text = self.get_end(all_texts, summary_text)\n",
    "        \n",
    "        if self.ix_end > self.ix_start:\n",
    "            self.vote_names, self.vote_page_numbers = self.parse_names_and_page_numbers(summary_text)\n",
    "        \n",
    "    def get_start(self, all_texts:typing.List[str]): \n",
    "        ix_start = get_ix(all_texts, self.is_toc)[0]        \n",
    "        return ix_start, all_texts[ix_start]\n",
    "        \n",
    "    def pypdf2_parse_names_and_page_numbers(self, all_texts:typing.List[str]): \n",
    "        texts = re.split(r'(\\d+\\.\\n?)(?=\\s*[a-zA-Z])', all_texts)\n",
    "        \n",
    "        ix_content = get_ix(texts, self.is_toc)[0]\n",
    "        texts = texts[ix_content+1:]\n",
    "\n",
    "        given_number = [text.strip() for text in texts[::2]]\n",
    "\n",
    "        contents = [text.strip() for text in texts[1::2]]\n",
    "        page_numbers, vote_names = [], []\n",
    "        \n",
    "        for text in contents:\n",
    "            if 'docx' in text:\n",
    "                _text = text.split('\\n')\n",
    "                _ix = get_ix(_text, lambda x: 'docx' in x)[0]\n",
    "                _text = '\\n'.join(_text[:_ix])\n",
    "            else:\n",
    "                _text = text\n",
    "            try:\n",
    "                re.search(r'(\\d+)$', _text).group()\n",
    "            except:\n",
    "                print('failed at parsing', [text], 'to', [_text])\n",
    "            page_numbers.append(int(re.search(r'(\\d+)$', _text).group()))\n",
    "            vote_names.append(re.sub(r'\\.*\\d+$', '', _text).strip())\n",
    "        \n",
    "        assert len(vote_names) == len(page_numbers)\n",
    "        assert len(set(page_numbers)) == len(page_numbers), collections.Counter(page_numbers).most_common()\n",
    "        \n",
    "        return vote_names, [nr-1 for nr in page_numbers]\n",
    "    \n",
    "    def pdfminer_parse_names_and_page_numbers(self, all_texts:typing.List[str]): \n",
    "        texts = re.split(r'(?<=\\d)\\n\\n', all_texts)\n",
    "        #print(texts)\n",
    "        \n",
    "        texts = [text for text in texts if 'docx' not in text and re.search(r'(\\d{3}\\.\\d{3})', text) is None]\n",
    "        #some strings do not properly begin with a number, rather it is somewhere in the middle\n",
    "        ix_messed_up = [i for i, text in enumerate(texts) if text[0].isalpha()]\n",
    "        #print(ix_messed_up)\n",
    "        print([texts[i] for i in ix_messed_up])\n",
    "        misplaced_numbers = [re.search(r'(\\n\\n\\d\\.\\n)', texts[i]).group() for i in ix_messed_up]\n",
    "        for i, number in zip(ix_messed_up, misplaced_numbers):\n",
    "            texts[i] = re.sub(r'(\\n\\n\\d\\.\\n)', ' ', texts[i])\n",
    "            texts[i] = number.lstrip() + texts[i]\n",
    "        print(texts)\n",
    "        \n",
    "        ix_content = get_ix(texts, self.is_toc)[0]\n",
    "        texts = texts[ix_content+1:]\n",
    "\n",
    "        given_number = [text.strip() for text in texts[::2]]\n",
    "\n",
    "        contents = [text.strip() for text in texts[1::2]]\n",
    "        #[text0.strip()+text1.strip() for text0, text1 in zip(texts[1::3],\n",
    "        #                                                                texts[2::3])]\n",
    "        #print(given_number)\n",
    "        #print(contents)\n",
    "        page_numbers, vote_names = [], []\n",
    "        \n",
    "        for text in contents:\n",
    "            if 'docx' in text:\n",
    "                print('\\ntööööööxt', text)\n",
    "                _text = text.split('\\n')\n",
    "                _ix = get_ix(_text, lambda x: 'docx' in x)[0]\n",
    "                _text = '\\n'.join(_text[:_ix])\n",
    "            else:\n",
    "                _text = text\n",
    "            try:\n",
    "                re.search(r'(\\d+)$', _text).group()\n",
    "            except:\n",
    "                print('failed at parsing', [text], 'to', [_text])\n",
    "            print(_text)    \n",
    "            page_numbers.append(int(re.search(r'(\\d+)$', _text).group()))\n",
    "            vote_names.append(re.sub(r'\\.*\\d+$', '', _text).strip())\n",
    "        \n",
    "        assert len(vote_names) == len(page_numbers)\n",
    "        assert len(set(page_numbers)) == len(page_numbers), collections.Counter(page_numbers).most_common()\n",
    "        \n",
    "        return vote_names, [nr-1 for nr in page_numbers]\n",
    "        \n",
    "    def parse_names_and_page_numbers(self, all_texts:typing.List[str]): \n",
    "        \n",
    "        return getattr(self, f'{self.method}_parse_names_and_page_numbers')(all_texts)\n",
    "        \n",
    "    def get_end(self, all_texts:typing.List[str], summary_text:str): \n",
    "        \n",
    "        if self.ix_start + 1 == self.vote_page_numbers[0]:\n",
    "            return self.ix_start, summary_text\n",
    "        \n",
    "        ix_end = self.vote_page_numbers[0]\n",
    "        summary_text = '\\n'.join(all_texts[self.ix_start:ix_end])\n",
    "        \n",
    "        return ix_end, summary_text\n",
    "    \n",
    "    @property\n",
    "    def df(self):\n",
    "        return pd.DataFrame({\n",
    "            'vote name': self.vote_names,\n",
    "            'start page': self.vote_page_numbers,\n",
    "            'end page': [nr - 1 for nr in self.vote_page_numbers[1:]] + [self.num_pages]\n",
    "        })\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = SummaryParser(all_texts, method='pdfminer')\n",
    "df_summary = summary.df\n",
    "df_summary.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = SummaryParser(all_texts2, method='pypdf2')\n",
    "df_summary = summary.df\n",
    "df_summary.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 150):\n",
    "    display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VotesParser:\n",
    "    \n",
    "    def __init__(self, start_page:int, end_page:int, all_texts:typing.List[str]):\n",
    "        '`end_page` is inclusive'\n",
    "        assert start_page <= end_page\n",
    "        self.start_page = start_page\n",
    "        self.end_page = end_page\n",
    "        \n",
    "        votes_texts = self.preprocess(all_texts)\n",
    "        \n",
    "        original_votes, vote_corrections = self.check_for_corrections(votes_texts)\n",
    "        \n",
    "        #print(original_votes, vote_corrections)\n",
    "        \n",
    "        self.vote_counts, self.votes = self.parse_votes(original_votes)\n",
    "        \n",
    "        if vote_corrections is not None:\n",
    "            self.corrections = self.parse_vote_corrections(vote_corrections)\n",
    "    \n",
    "    @property\n",
    "    def df_votes(self):\n",
    "        df = []\n",
    "        for outcome in self.votes:\n",
    "            for party in self.votes[outcome]:\n",
    "                df.extend([(mep, outcome, party) for mep in self.votes[outcome][party]])\n",
    "        return pd.DataFrame(df, columns=['MEP', 'vote', 'Party'])\n",
    "    \n",
    "    @property\n",
    "    def df_corrections(self):\n",
    "        df = []\n",
    "        for outcome in self.votes:\n",
    "            df.extend([(mep, outcome) for mep in self.corrections[outcome]])\n",
    "        return pd.DataFrame(df, columns=['MEP', 'vote'])\n",
    "    \n",
    "    def preprocess(self, all_texts:typing.List[str]):\n",
    "        votes_texts = \"\\n\".join(all_texts[self.start_page:self.end_page+1])\n",
    "        \n",
    "        s = slice(self.start_page, self.end_page+1)\n",
    "        votes_texts = \"\\n\".join(all_texts[s]).split(\"\\n\")\n",
    "        \n",
    "        votes_texts = [text for text in votes_texts if \\\n",
    "                       useful_string(text)]\n",
    "        return votes_texts\n",
    "    \n",
    "    def parse_votes(self, votes_texts:typing.List[str]):\n",
    "        #print(votes_texts)\n",
    "        \n",
    "        ix_yes, ix_no, ix_abstain = self.get_yes_no_abstain_indices(votes_texts)\n",
    "        \n",
    "        counts = {\n",
    "            'yes': int(votes_texts[ix_yes-1]),\n",
    "            'no': int(votes_texts[ix_no-1]),\n",
    "            'abstain': int(votes_texts[ix_abstain-1]),\n",
    "        }\n",
    "        votes = {\n",
    "            'yes': self.get_mep_and_party_vote_for_outcome(votes_texts[ix_yes:ix_no-1]),\n",
    "            'no': self.get_mep_and_party_vote_for_outcome(votes_texts[ix_no:ix_abstain-1]),\n",
    "            'abstain': self.get_mep_and_party_vote_for_outcome(votes_texts[ix_abstain:])\n",
    "        }\n",
    "        #print(counts)\n",
    "        #print([(k, (x:=[len(val) for val in v.values()]), sum(x)) for k,v in votes.items()])\n",
    "        return counts, votes\n",
    "    \n",
    "    def get_yes_no_abstain_indices(self, votes_texts):\n",
    "        ix_yes = min(get_ix(votes_texts, lambda x: x == '+'))\n",
    "        ix_no = get_ix(votes_texts, lambda x: x == '-')[0]\n",
    "        ix_abstain = max(get_ix(votes_texts, lambda x: x == '0'))\n",
    "        assert ix_yes > 0\n",
    "        assert ix_yes < ix_no\n",
    "        assert ix_no < ix_abstain\n",
    "        return ix_yes, ix_no, ix_abstain\n",
    "    \n",
    "    def parse_vote_corrections(self, votes_corrections_texts:typing.List[str]): \n",
    "\n",
    "        ix_yes, ix_no, ix_abstain = self.get_yes_no_abstain_indices(votes_corrections_texts)\n",
    "        \n",
    "        yes_meps = '\\n'.join(votes_corrections_texts[ix_yes+1:ix_no]).split(',')\n",
    "        no_meps = '\\n'.join(votes_corrections_texts[ix_no+1:ix_abstain]).split(',')\n",
    "        abstain_meps = '\\n'.join(votes_corrections_texts[ix_abstain+1:]).split(',')\n",
    "        \n",
    "        return {\n",
    "            'yes': [self.process_mep(mep) for mep in yes_meps if useful_string(mep.strip())],\n",
    "            'no': [self.process_mep(mep) for mep in no_meps if useful_string(mep.strip())],\n",
    "            'abstain': [self.process_mep(mep) for mep in abstain_meps if useful_string(mep.strip())]\n",
    "        }\n",
    "        \n",
    "    def get_mep_and_party_vote_for_outcome(self, votes_texts:typing.List[str]):\n",
    "\n",
    "        ix_party = [(i,text) for i, text in enumerate(votes_texts)\\\n",
    "                    if len(text)>0 and (':' == text[0] or ':' == text[-1])]\n",
    "        \n",
    "        ix_party = [i if (text[-1]==':' and len(text)>1) else i-1 for i, text in ix_party]\n",
    "        parties = [votes_texts[ix] for ix in ix_party]\n",
    "        meps = [','.join(votes_texts[ix0+1:ix1]).split(',') for ix0, ix1 in \n",
    "                zip(ix_party, ix_party[1:]+[len(votes_texts)])]\n",
    "        votes = {party: [self.process_mep(mep) for mep in _meps if useful_string(mep.strip())] for party, _meps in zip(parties, meps)}\n",
    "        \n",
    "        return votes\n",
    "    \n",
    "    def process_mep(self, mep:str, placeholder:str='unknown mep'):\n",
    "        mep = mep.strip()\n",
    "        if ':' in mep:\n",
    "            if mep.startswith(':'):\n",
    "                mep = mep[1:]\n",
    "            if mep.endswith(':'):\n",
    "                mep = mep[:-1]\n",
    "        if mep == '':\n",
    "            mep = placeholder\n",
    "        return mep\n",
    "        \n",
    "    def check_for_corrections(self, votes_texts:typing.List[str]): \n",
    "        \n",
    "        ix = get_ix(votes_texts, lambda x: 'CORRECCIONES E INTENCIONES DE VOTO' in x)\n",
    "        assert len(ix) <= 1\n",
    "        no_corrections = len(ix) == 0\n",
    "        if no_corrections:\n",
    "            return votes_texts, None\n",
    "        else:\n",
    "            return votes_texts[:ix[0]], votes_texts[ix[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start, end = summary.df.iloc[0,1:]\n",
    "votes = VotesParser(start, end, all_texts)\n",
    "display(votes.df_votes, votes.df_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = summary.df\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing all issues voted on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_all_issues(df_summary:pd.DataFrame, all_texts:typing.List[str]):\n",
    "    all_votes, all_corrections = [], []\n",
    "    all_counts = []\n",
    "\n",
    "    for i,row in df_summary.iterrows():\n",
    "        votes = VotesParser(row['start page'], row['end page'], all_texts)\n",
    "\n",
    "        df_votes = votes.df_votes\n",
    "        df_votes['vote name'] = row['vote name']\n",
    "        all_votes.append(df_votes)\n",
    "\n",
    "        counts = votes.vote_counts\n",
    "        counts['vote name'] = row['vote name']\n",
    "        all_counts.append(counts)\n",
    "\n",
    "        df_corrections = votes.df_corrections\n",
    "        df_corrections['vote name'] = row['vote name']\n",
    "        all_corrections.append(df_corrections)\n",
    "\n",
    "    all_counts = pd.DataFrame(all_counts)\n",
    "    all_votes = pd.concat(all_votes)\n",
    "    all_corrections = pd.concat(all_corrections)\n",
    "    return all_counts, all_votes, all_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_counts, all_votes, all_corrections = get_all_issues(df_summary, all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts.head(), all_counts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_votes.head(), all_votes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap between vote names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = set(all_counts['vote name'].unique())\n",
    "s2 = set(all_votes['vote name'].unique())\n",
    "\n",
    "assert len(s1.intersection(s2))/len(s1.union(s2)) == 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of total counts vs identified counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_votes_summed = all_counts.copy()\n",
    "outcomes = ['yes', 'no', 'abstain']\n",
    "all_votes_summed.loc[:,outcomes] = None\n",
    "\n",
    "for outcome in outcomes:\n",
    "    vote_mask = all_votes['vote'] == outcome\n",
    "    \n",
    "    for vote in all_votes['vote name'].unique():\n",
    "        mask = all_votes_summed['vote name'] == vote\n",
    "        _votes = (all_votes['vote name'] == vote) & vote_mask\n",
    "        all_votes_summed.loc[mask, outcome] = _votes.sum()\n",
    "    \n",
    "    all_votes_summed[outcome] = all_votes_summed[outcome].astype(int)\n",
    "    all_votes_summed[f'{outcome}-delta'] = all_votes_summed[outcome] - all_counts[outcome]\n",
    "        \n",
    "all_votes_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_votes_summed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(all_votes_summed, value_vars=['yes','no','abstain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = pd.melt(all_votes_summed, value_vars=[v for v in all_votes_summed.columns if 'delta' in v])\n",
    "deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(deltas.groupby('variable').describe())\n",
    "px.histogram(data_frame=deltas,\n",
    "             x=f'value',\n",
    "             color='variable',\n",
    "             barmode='overlay', nbins=21,\n",
    "             title=f'Discrepancy for \\'{outcome}\\' votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_pdf_docs]",
   "language": "python",
   "name": "conda-env-py38_pdf_docs-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
